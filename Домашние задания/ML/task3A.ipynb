{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, DS-поток\n",
    "## Домашнее задание 3A\n",
    "\n",
    "**Правила:**\n",
    "\n",
    "* Дедлайн **09 марта 23:59**. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Выполненную работу нужно отправить на почту ` mipt.stats@yandex.ru`, указав тему письма `\"[ml] Фамилия Имя - задание 3A\"`. Квадратные скобки обязательны. Если письмо дошло, придет ответ от автоответчика.\n",
    "* Прислать нужно ноутбук и его pdf-версию (без архивов). Названия файлов должны быть такими: `3A.N.ipynb` и `3A.N.pdf`, где `N` - ваш номер из таблицы с оценками.\n",
    "* Решения, размещенные на каких-либо интернет-ресурсах не принимаются. Кроме того, публикация решения в открытом доступе может быть приравнена к предоставлении возможности списать.\n",
    "* Для выполнения задания используйте этот ноутбук в качествие основы, ничего не удаляя из него.\n",
    "* Никакой код из данного задания при проверке запускаться не будет.\n",
    "\n",
    "**Задание стоит 15 баллов.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(font_scale=1.6)\n",
    "plt.rcParams['axes.facecolor'] = 'lightgrey'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, как именно происходит построение решающего дерева. Для построения дерева в каждой нелистовой вершине происходит разбиение подвыборки на две части по некоторому признаку $x_j$. Этот признак и порог $t$, по которому будет происходить разбиение, мы хотим брать не произвольно, а основываясь на соображениях оптимальности.  Для этого нам необходимо знать некоторый фукционал качества, который будем оптимизировать при построении разбиения. \n",
    "\n",
    "Обозначим через $X_m$ -- множество объектов, попавших в вершину $m$, разбиваемую на данном шаге, а через $X_l$ и $X_r$ -- объекты, попадающие в левое и правое поддерево соответственно при заданном правиле $I\\{x_j < t\\}$. Пусть также $H$ -- используемый критерий информативности (impurity criterion).\n",
    "\n",
    "Выпишите функционал, который необходимо минимизировать при разбиении вершины:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ:** <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализация критериев информативности.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомните еще раз, на какой общей идее основаны критерии информативности и какую характеристику выборки они стремятся оптимизировать?\n",
    "\n",
    "**Ответ:** <...>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как непосредственно работать с решающими деревьями, реализуйте функции подсчёта значения критериев разбиения вершин решающих деревьев. Использовать готовые реализации критериев или классов для решающих деревьев из `sklearn` и из других библиотек **запрещено.** Также при реализации критериев по причине неэффективности **запрещается использовать циклы**. Воспользуйтесь библиотекой `numpy`.\n",
    "\n",
    "Каждая функция принимает на вход одномерный `numpy`-массив размерности `(n,)` из значений отклика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код функций, реализующих критерии разбиения. \n",
    "\n",
    "def mean_square_criterion(y):\n",
    "    ''' Критерий для квадратичной функции потерь. '''\n",
    "    \n",
    "    return <...>\n",
    "\n",
    "\n",
    "def mean_abs_criterion(y):\n",
    "    ''' Критерий для абсолютной функции потерь. '''\n",
    "    \n",
    "    return <...>\n",
    "\n",
    "\n",
    "def get_probs_by_y(y):\n",
    "    ''' Возвращает вектор частот для каждого класса выборки. '''\n",
    "\n",
    "    return <...>\n",
    "\n",
    "\n",
    "def gini_criterion(y):\n",
    "    ''' Критерий Джини. '''\n",
    "    \n",
    "    return <...>\n",
    "\n",
    "\n",
    "def entropy_criterion(y):\n",
    "    ''' Энтропийный критерий. '''\n",
    "    \n",
    "    return <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте реализованные функции.\n",
    "\n",
    "Тесты для распределения вероятностей на классах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(get_probs_by_y([1, 1, 2, 2, 7]), np.array([0.4, 0.4, 0.2]))\n",
    "assert np.allclose(get_probs_by_y([1]), np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты для критериев разбиения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(entropy_criterion([25]), 0)\n",
    "assert np.allclose(gini_criterion([25]), 0)\n",
    "assert np.allclose(mean_square_criterion([10, 10, 10]), 0)\n",
    "assert np.allclose(mean_abs_criterion([10, 10, 10]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализация класса решающего дерева.**\n",
    "\n",
    "Для того, чтобы лучше понять, как устроены решающие деревья и как именно устроен процесс их построения, вам предлагается реализавать класс `BaseDecisionTree`, реализующий базовые функции решающего дерева.  Большая часть кода уже написана. \n",
    "\n",
    "Используются следующие классы:\n",
    "\n",
    "**Класс** `BaseDecisionTree` - класс для решающего дерева, в котором реализовано построение дерева. Все вершины дерева хранятся в списке `self.nodes`, при этом вершина с номером 0 - корень.\n",
    "\n",
    "\n",
    "1) `__init__` - инициализация дерева. Здесь сохраняются гиперпараметры дерева: `criterion`, `max_depth`, `min_samples_split` и инициализируется список вершин, состоящий только из одной вершины - корневой,\n",
    "\n",
    "2) `build_` - рекурсивная функция построения дерева. В ней при посещении каждой вершины дерева проверяются условия, стоит ли продолжать разбивать эту вершину. Если да, то перебираются все возможные признаки и пороговые значения и выбирается та пара (признак, значение), которой соответствует наименьшее значение критерия информативности,\n",
    "\n",
    "3) `fit` - функция обучения дерева, принимающая на вход обучающую выборку. В этой функции происходит предподсчёт всех возможных пороговых значений для каждого из признаков, а затем вызывается функция `build_`.\n",
    "\n",
    "**Класс** `Node` - класс вершины дерева. Внутри вершины, помимо раздяляющего признака и порога хранятся `self.left_son`, `self.right_son` - номера дочерних вершин, а также `self.left_prob` и `self.right_prob` - вероятности попадания элемента в каждую из них. При этом в листовых вершиных хранятся также `self.y_values` - значения соответствующих элементов выборки, попавших в вершину.\n",
    "\n",
    "1) `__init__` - инициализация вершины. Принимает в качестве аргументов разделяющий признак и пороговое значение и сохраняет их.\n",
    "\n",
    "**Класс** `DecisionTreeRegressor` - наследник класса `BaseDecisionTree`, в котором реализованы функции для предсказаний при решении задачи регрессии.\n",
    "\n",
    "1) `predict_instance` - получение предсказания для одного элемента выборки. Выполняется посредством спуска по решающему дереву до листовой вершины,\n",
    "\n",
    "2) `predict` - получение предсказаний для всех элементов выборки.\n",
    "\n",
    "\n",
    "**Класс** `DecisionTreeClassifier` - наследник класса `BaseDecisionTree`, в котором реализованы функции для предсказаний при решении задачи классификации.\n",
    "\n",
    "1) `predict_proba_instance` - предсказание распределения вероятностей по классам для одного элемента выборки, \n",
    "\n",
    "2) `predict_proba` - предсказание распределения вероятностей по классам для всех элементов выборки,\n",
    "\n",
    "3) `predict_proba` - предсказание меток классов для всех элементов выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед написанием кода разбиения дерева, ответьте на вопрос, какие пороговые значения для каждого из признаков вы будете перебирать. Почему рассматривать другие значения в качестве пороговых не имеет смысла?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ:** <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Структура решающего дерева**\n",
    "\n",
    "<img src=\"https://sun1-84.userapi.com/VGHHeEihyIdwbnZMCDLMoIBXyIq_k3U77KJoFQ/fBDwbb5VHNQ.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_nans(arr):\n",
    "    '''\n",
    "    Функция, которая создаёт и возвращает новый массив \n",
    "    из всех элементов переданного массива, не являющихся None.\n",
    "    '''\n",
    "    \n",
    "    return arr.copy()[arr != None]\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, split_feature=None, split_threshold=None):\n",
    "        '''\n",
    "        Функция инициализации вершины решающего дерева.\n",
    "        \n",
    "        Параметры.\n",
    "        1) split_feature - номер разделяющего признака\n",
    "        2) split_threshold - пороговое значение\n",
    "        '''\n",
    "        \n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        # По умолчанию считаем, что у вершины нет дочерних вершин.\n",
    "        self.left_son, self.right_son = None, None\n",
    "        # Вероятности попадания в каждую из дочерних вершин нужно поддерживать \n",
    "        # для корректной обработки данных с пропусками\n",
    "        self.left_prob, self.right_prob = 0, 0\n",
    "        # Массив значений y. Определён только для листовых вершин дерева\n",
    "        self.y_values = None\n",
    "        \n",
    "\n",
    "class BaseDecisionTree(BaseEstimator):\n",
    "    '''\n",
    "    Здесь содержится реализация всех основных функций для работы\n",
    "    с решающим деревом.\n",
    "    \n",
    "    Наследование от класса BaseEstimator нужно для того, чтобы \n",
    "    в дальнейшем данный   класс можно было использовать в \n",
    "    различных функциях библиотеки sklearn, например, в функциях \n",
    "    для кросс-валидации.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, criterion, max_depth=np.inf, min_samples_split=2):\n",
    "        '''\n",
    "        Функция инициализации решающего дерева.\n",
    "        \n",
    "        Параметры.\n",
    "        1) criterion - критерий информативности, \n",
    "        2) max_depth - максимальная глубина дерева,\n",
    "        3) min_samples_split - минимальное количество элементов \n",
    "        обучающей выборки,  которое должно попасть в вершину, \n",
    "        чтобы потом происходило разбиение этой вершины.\n",
    "        '''\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # Список всех вершин дерева. В самом начале \n",
    "        # работы алгоритма есть только одна \n",
    "        # вершина - корень.\n",
    "        self.nodes = [Node()]\n",
    "        # Количество классов. Актуально только \n",
    "        # при решении задачи классификации.\n",
    "        self.class_count = 1\n",
    "        # Сюда нужно будет записать все значения \n",
    "        # для каждого из признаков датасета.\n",
    "        self.feature_values = None\n",
    "    \n",
    "    def build_(self, v, X, y, depth=0):\n",
    "        '''\n",
    "        Рекурсивная функция построения решающего дерева.\n",
    "        \n",
    "        Параметры.\n",
    "        1) v - номер рассматриваемой вершины\n",
    "        2) X, y - обучающая выборка, попавшая в текущую вершину\n",
    "        3) depth - глубина вершины с номером v\n",
    "        '''\n",
    "        \n",
    "        if depth == self.max_depth or len(y) < self.min_samples_split:\n",
    "            # Еcли строим дерево для классификации, то \n",
    "            # сохраняем метки классов всех элементов выборки,\n",
    "            # попавших в вершину.\n",
    "            if callable(getattr(self, \"set_class_count\", None)):\n",
    "                self.nodes[v].y_values = y.copy()\n",
    "            # Для регрессии сразу вычислим среднее всех \n",
    "            # элементов вершины.\n",
    "            else:\n",
    "                self.nodes[v].y_values = np.mean(y)\n",
    "            return\n",
    "        \n",
    "        best_criterion_value = np.inf\n",
    "        best_feature, best_threshold = 0, 0\n",
    "        sample_size, feature_count = X.shape\n",
    "        \n",
    "        # переберём все возможные признаки и значения порогов,\n",
    "        # найдём оптимальный признак и значение порога \n",
    "        # и запишем их в best_feature, best_threshold\n",
    "        for feature_id in range(feature_count):\n",
    "            for threshold in self.feature_values[feature_id]:\n",
    "                <...>\n",
    "            \n",
    "        # сохраним найденные параметры в класс текущей вершины\n",
    "        self.nodes[v].split_feature = <...>\n",
    "        self.nodes[v].split_threshold = <...>\n",
    "        # разделим выборку на 2 части по порогу\n",
    "        <..>\n",
    "        \n",
    "        # создаём левую и правую дочерние вершины,\n",
    "        # и кладём их в массив self.nodes \n",
    "        self.nodes.append(Node())\n",
    "        self.nodes.append(Node())\n",
    "        # сохраняем индексы созданных вершин в качестве \n",
    "        # левого и правого сына вершины v\n",
    "        self.nodes[v].left_son, self.nodes[v].right_son =\\\n",
    "            len(self.nodes)-2, len(self.nodes)-1\n",
    "        # рекурсивно строим дерево для дочерних вершин\n",
    "        self.build_(self.nodes[v].left_son, X_l, y_l, depth+1)\n",
    "        self.build_(self.nodes[v].right_son, X_r, y_r, depth+1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Функция, из которой запускается построение \n",
    "        решающего дерева по обучающей выборке.\n",
    "        \n",
    "        Параметры.\n",
    "        X, y - обучающая выборка\n",
    "        '''\n",
    "        \n",
    "        # сохраним заранее все пороги для каждого из \n",
    "        # признаков обучающей выборки\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.feature_values = []\n",
    "        for feature_id in range(X.shape[1]):\n",
    "            self.feature_values.append(\n",
    "                np.unique(get_not_nans(X[:, feature_id]))\n",
    "            )\n",
    "            \n",
    "        set_class_count = getattr(self, \"set_class_count\", None)\n",
    "        # если строится дерево для классификации, \n",
    "        # то нужно посчитать количество классов \n",
    "        if callable(set_class_count):\n",
    "            set_class_count(y)\n",
    "        self.build_(0, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда общий код решающего дерева написан, нужно сделать обёртки над `BaseDecisionTree` - классы `DecisionTreeRegressor` и `DecisionTreeClassifier` для использования решающего дерева в задачах регрессии и классификации соответственно.\n",
    "\n",
    "Допишите функции `predict_instance` и `predict_proba_instance` в классах для регрессии и классификации соответственно. В этих функциях нужно для одного элемента $x$ выборки промоделировать спуск в решающем дереве, а затем по листовой вершине, в которой окажется объект, посчитать для классификации - распределение вероятностей, а для регрессии - число $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(BaseDecisionTree):\n",
    "    def predict_instance(self, x, v):\n",
    "        '''\n",
    "        Рекурсивная функция, предсказывающая значение\n",
    "        y для одного элемента x из выборки.\n",
    "        \n",
    "        Параметры.\n",
    "        1) x - элемент выборки, для которого\n",
    "        требуется предсказать значение y\n",
    "        2) v - рассматриваемая вершина дерева\n",
    "        '''\n",
    "        \n",
    "        # если вершина - лист, возвращаем в качестве предсказания \n",
    "        # среднее всех элементов, содержащихся в ней\n",
    "        if self.nodes[v].left_son == None:\n",
    "            <...>\n",
    "            \n",
    "        # если у объекта x значение признака по\n",
    "        # которому происходит разделение,  меньше \n",
    "        # порогового, то спускаемся в левое поддерево,\n",
    "        # иначе - в правое\n",
    "        if x[self.nodes[v].split_feature] < self.nodes[v].split_threshold:\n",
    "            return <...>\n",
    "        elif x[self.nodes[v].split_feature] >= self.nodes[v].split_threshold:\n",
    "            return <...>\n",
    "        # а если у элемента отсутствует значение \n",
    "        # разделяющего признака,  то будем спускаться \n",
    "        # в оба поддерева\n",
    "        else:\n",
    "            left_predict = self.predict_instance(x, self.nodes[v].left_son)\n",
    "            right_predict = self.predict_instance(x, self.nodes[v].right_son)\n",
    "            return <...>\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Функция, предсказывающая значение\n",
    "        y для всех элементов выборки X.\n",
    "        \n",
    "        Параметры.\n",
    "        X - выборка, для которой требуется\n",
    "        получить вектор предсказаний y\n",
    "        '''\n",
    "        \n",
    "        return [self.predict_instance(x, 0) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства реализации функции `predict_proba_instance` класса `DecisionTreeClassifier` будем считать, что все классы имеют целочисленные метки от 0 до $k-1$, где $k$ - количество классов. Если бы это условие не было выполнено, то нужно было бы сначала сделать предобработку меток классов в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(BaseDecisionTree):\n",
    "    def set_class_count(self, y):\n",
    "        '''\n",
    "        Функция, вычисляющая количество классов \n",
    "        в обучающей выборке.\n",
    "        \n",
    "        Параметры.\n",
    "        y - значения класса в обучающей выборке\n",
    "        '''\n",
    "        \n",
    "        self.class_count = np.max(y) + 1\n",
    "    \n",
    "    def predict_proba_instance(self, x, v):\n",
    "        '''\n",
    "        Рекурсивная функция, предсказывающая вектор\n",
    "        вероятностей принадлежности  объекта x\n",
    "        к классам\n",
    "        \n",
    "        Параметры.\n",
    "        1) x - элемент выборки, для которого \n",
    "        требуется предсказать значение y\n",
    "        2) v - вершина дерева, в которой \n",
    "        находится алгоритм\n",
    "        '''\n",
    "        \n",
    "        if self.nodes[v].left_son == None:\n",
    "            result = np.zeros(self.class_count)\n",
    "            classes, counts = np.unique(\n",
    "                self.nodes[v].y_values, return_counts=True)\n",
    "            result[classes.astype(int)] = counts\n",
    "            return result / np.sum(result)\n",
    "            \n",
    "        # если у объекта x значение признака по которому\n",
    "        # происходит разделение, меньше порогового, \n",
    "        # то спускаемся в левое поддерево, иначе - в правое\n",
    "        if x[self.nodes[v].split_feature] < self.nodes[v].split_threshold:\n",
    "            return <...>\n",
    "        elif x[self.nodes[v].split_feature] >= self.nodes[v].split_threshold:\n",
    "            return <...>\n",
    "        # а если у объекта отсутствует значение \n",
    "        # разделяющего признака, то будем спускаться \n",
    "        # в оба поддерева\n",
    "        else:\n",
    "            left_predict = self.predict_proba_instance(\n",
    "                x, self.nodes[v].left_son)\n",
    "            right_predict = self.predict_proba_instance(\n",
    "                x, self.nodes[v].right_son)\n",
    "            return <...>\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Функция, предсказывающая вектор вероятностей\n",
    "        принадлежности объекта x к классам для \n",
    "        каждого x из X\n",
    "        \n",
    "        Параметры.\n",
    "        X - выборка, для которой требуется получить вектор предсказаний y\n",
    "        '''\n",
    "        \n",
    "        return [self.predict_proba_instance(x, 0) for x in X]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Функция, предсказывающая метку класса для\n",
    "        всех элементов выборки X.\n",
    "        \n",
    "        Параметры.\n",
    "        X - выборка, для которой требуется получить\n",
    "        вектор предсказаний y\n",
    "        '''\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подбор параметров.**\n",
    "\n",
    "В этой части задания вам предлагается поработать с написанным решающим деревом, применив его к задачи классификации и регрессии, и в обеих задачах подобрать оптимальные параметры для построения.\n",
    "\n",
    "Не забывайте писать выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Задача классификации.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь - самое время протестировать работу написанного нами решающего дерева. Делать мы это будем на датасете для классификации вина из `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для критерия Джини и энтропийного критерия найдем оптимальные параметры обучения дерева - `max_depth` и `min_samples_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_criteria = <...>\n",
    "criterion_names = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С начала надо разбить выборку на train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проведите кросс-валидацию для каждого из критериев разбиения вершин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for criterion, criterion_name in zip(classification_criteria, criterion_names):\n",
    "    accuracy = <...>\n",
    "    print('accuracy:', accuracy)\n",
    "    assert(accuracy >= 0.85, \"Something is wrong with your classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Построение графиков.**\n",
    "\n",
    "Постройте графики зависимости accuracy от максимальной глубины дерева на обучающей и тестовой выборке для каждого критерия на train и на test. В качестве максимальной глубины используйте значения от 3 до 7. Значение `min_samples_split` фиксируйте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код построения графиков\n",
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте выводы. Почему графики получились такими? Как соотносятся оптимальные значения параметров на обучающей и на тестовой выборках?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.**\n",
    "\n",
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Задача регрессии.**\n",
    "\n",
    "Проделайте аналогичные шаги для задачи регрессии. В качестве датасете возьмите `boston` из `sklearn`, а в качестве критерия качества возьмите r2_score. Рассмотрим более широкий диапозон значений для `max_depth`: от 3 до 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_X, boston_y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_criteria = <...>\n",
    "criterion_names = <...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём выборку на обучение и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты, аналогичны тем, что были сделаны для задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте вывод, в котором объясните, почему графики получились такими.\n",
    "\n",
    "Скорее всего, вы заметили, что дерево в этих экспериментах строится довольно медленно. Как можно ускорить его построение? Можно ли ускорить нахождение оптимального разбиения по некоторому вещественному признаку?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.**\n",
    "\n",
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка пропусков с использованием решающих деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь рассмотрим датасет, в котором часть данных пропущена. В качестве примера возьмём датасет https://archive.ics.uci.edu/ml/datasets/Adult для определения категории дохода работников, по таким признакам, как возраст, образование, специальность, класс работы, пол, кол-во отрабатываемых часов в неделю и некоторым другим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education1', 'education2', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
    "    'capital-loss', 'hours-per-week', 'native-country', 'target'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку предсказание в дереве на данных с пропусками часто занимает сильно больше времени, чем в случае отсутствия пропусков (так как часто приходится спускаться разу в 2 поддерева), то для экономии времени сократим датасет, взяв из него только первые 10000 строк данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv('adult.data', header=None)[:10000]\n",
    "adult_df.columns = column_names\n",
    "target = adult_df['target'] == ' >50K'\n",
    "adult_df = adult_df.drop(['target'], axis=1)\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем датасет, заменив категориальные признаки one-hot векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.get_dummies(adult_df)\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку все пропущенные значения относились к категориальным признакам и помечались в датасете знаком `?`, то для каждого категориального признака `feature` исходного датасета надо выполнить следующую процедуру: рассмотреть признак `feature_?` нового датасета и для всех строк, для которых выполнено `feature_?=1`, значениях всех признаков с префиксом `feature` установить в `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(adult_df.shape[0])\n",
    "\n",
    "for feature in column_names:\n",
    "    if f'{feature}_ ?' in adult_df.columns:\n",
    "        none_indices = all_indices[adult_df[f'{feature}_ ?'] == 1]\n",
    "        \n",
    "        for dummy_feature in adult_df.columns:\n",
    "            if not dummy_feature.startswith(f'{feature}_ '):\n",
    "                continue\n",
    "            if dummy_feature != f'{feature}_ ?':\n",
    "                adult_df[dummy_feature][none_indices] = None\n",
    "        adult_df = adult_df.drop(f'{feature}_ ?', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение пропущенных значений по признакам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(adult_df.isnull(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём данные на обучающую и тестовую выборки в отношении 3:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adult_train, X_adult_test, y_adult_train, y_adult_test = train_test_split(\n",
    "    adult_df, target, random_state=777\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи кросс-валидации найдём оптимальные гиперпараметры для каждого из критериев разбиения деревьев для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты c построением графиков, аналогичные тем, что были сделаны в предыдущем пункте для задач классификации и регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.**\n",
    "\n",
    "<...>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
